{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10939578,"sourceType":"datasetVersion","datasetId":6802811}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Required Modules\n\nimport os\nimport IPython.display as ipd\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom tensorflow import math\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Rescaling, Conv2D, MaxPooling2D\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.optimizers import Adam, SGD, Adagrad, RMSprop\nfrom tensorflow.keras.models import load_model\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T06:58:06.120341Z","iopub.execute_input":"2025-03-10T06:58:06.120546Z","iopub.status.idle":"2025-03-10T06:58:09.101569Z","shell.execute_reply.started":"2025-03-10T06:58:06.120527Z","shell.execute_reply":"2025-03-10T06:58:09.100926Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/birdfiltered/10SecDatasets/10SecDatasets'\nBIRDS = os.listdir(f'{DATA_PATH}/Train')\n# BIRDS = [ 'Barn_Swallow',\n#     'Black-backed_Puffback',\n#     'Black_Kite',\n#     'Cape_Robin-Chat',\n#     'Cattle_Egret',\n#     'Collared_Sunbird',\n#     'Common_Bulbul',\n#     'Common_Buzzard',\n#     'Common_House-Martin',/\n#     'Common_Sandpiper',\n#     'Egyptian_Goose',\n#     'Eurasian_Hoopoe',\n#     'European_Bee-eater',\n#     'Gray-backed_Camaroptera',\n#     'Great_Egret',\n#     'Little_Egret',\n#     'Rattling_Cisticola',\n#     'Red-backed_Scrub-Robin',\n#     'Red-rumped_Swallow',\n#     'Sombre_Greenbul',\n#     'Tawny-flanked_Prinia',\n#     'Thrush_Nightingale',\n#     'Western_Yellow_Wagtail',\n#     'Willow_Warbler',\n#     'Wood_Sandpiper']\n\nBATCH_SIZE =8\nIM_SIZE = (224, 224) \nNO_CLASSES = len(BIRDS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T06:58:09.103333Z","iopub.execute_input":"2025-03-10T06:58:09.103810Z","iopub.status.idle":"2025-03-10T06:58:09.124584Z","shell.execute_reply.started":"2025-03-10T06:58:09.103785Z","shell.execute_reply":"2025-03-10T06:58:09.123784Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load the train, validation and testing dataset\n\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\n# Data augmentation\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    fill_mode=\"nearest\",\n)\ntrain_batches = train_datagen.flow_from_directory(\n    os.path.join(DATA_PATH, \"Train\"),\n    classes=BIRDS,\n    target_size=IM_SIZE,\n    class_mode=\"categorical\",\n    shuffle=True,\n    batch_size=BATCH_SIZE,\n)\n\nvalid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                  rescale=1./255)\nvalid_batches = valid_datagen.flow_from_directory(\n    os.path.join(DATA_PATH, \"Validation\"),\n    classes=BIRDS,\n    target_size=IM_SIZE,\n    class_mode=\"categorical\",\n    shuffle=False,\n    batch_size=BATCH_SIZE,\n)\n\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                 rescale=1./255)\ntest_batches = test_datagen.flow_from_directory(\n    os.path.join(DATA_PATH, \"Test\"),\n    classes=BIRDS,\n    target_size=IM_SIZE,\n    class_mode=\"categorical\",\n    shuffle=False,\n    batch_size=BATCH_SIZE,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T06:58:09.125790Z","iopub.execute_input":"2025-03-10T06:58:09.126034Z","execution_failed":"2025-03-10T06:58:28.819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import EfficientNetB0\n\nlr = 0.0001\nloss = \"categorical_crossentropy\"\n\n# Transfer learning EfficientNetB3\nmodel = EfficientNetB0(\n    include_top=False, weights=\"imagenet\", input_tensor=None, input_shape=(IM_SIZE[0],IM_SIZE[1], 3)\n)\nx = model.output\nx = Flatten()(x)\nx = Dense(1000, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(256, activation='relu')(x)\noutput_layer = Dense(len(BIRDS), activation=\"softmax\", name=\"softmax\")(x)\nmodel = Model(inputs=model.input, outputs=output_layer)\nmodel.compile(\n    optimizer=Adam(learning_rate=lr), loss=loss, metrics=[\"accuracy\"]\n)\n\n# print(model.summary())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define callbacks\n\nimport datetime\n# Model checkpoint\nlogs_directory = 'model/logs/'\ncheckpoint = ModelCheckpoint(\n    r\"model/model_checkpoint.keras\",\n    monitor=\"val_loss\",\n    verbose=0,\n    save_best_only=True,\n    mode=\"auto\",\n    save_freq='epoch',\n)\n\n# ReduceLR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=3e-4)\n# Function for exponential learning rate decay after 5 epochs\ndef scheduler(epoch, lr):\n  decay = -0.1\n  if epoch < 5:\n    return lr\n  else:\n    return float(lr * math.exp((epoch+1)*decay))\n\nlrscheduler = LearningRateScheduler(scheduler)\n\n# logdir = os.path.join(\"logsMFCC\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n# tensorboard = TensorBoard(log_dir=logdir,histogram_freq=1)\n\n# Early stopping if val_loss does not decrease for 3 epochs\nearly_stopping_monitor = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True)\n\ncallbacks = [checkpoint, lrscheduler, early_stopping_monitor]\n# callbacks = [tensorboard]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EPOCHS = 30\n# steps_per_epoch = train_batches.samples // BATCH_SIZE\n# validation_steps = valid_batches.samples // BATCH_SIZE\n\n# history = model.fit(\n#     train_batches,\n#     validation_data=valid_batches,\n#     epochs=EPOCHS,\n#     callbacks=callbacks,\n#     steps_per_epoch=steps_per_epoch,\n#     validation_steps=validation_steps,\n#     verbose=1  # Change to 2 for more compact output\n# )\n\n# # Optionally save the training history\n# import json\n# with open('training_history.json', 'w') as f:\n#     json.dump(history.history, f)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\n\nEPOCHS = 30\nhistory = model.fit(\n    train_batches,\n    validation_data=valid_batches,\n    epochs=EPOCHS,\n    callbacks= callbacks)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('./model/EFF_v5.keras')\nmodel_loaded = load_model('./model/EFF_v5.keras')\nmodel_loaded.evaluate(test_batches)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate(test_batches)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n# Confusion Matrix\n\ndef confusion_matrix_plot(model, test_batches):\n    \n    annot_size = 24\n    fig_size = (20, 20)\n\n    preds = model_loaded.predict(x=test_batches)\n    y_preds = np.argmax(preds, axis=-1)\n    y_true = test_batches.classes\n\n    confuse = confusion_matrix(y_true, y_preds)\n\n    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n    sns.set(rc = {'figure.figsize':fig_size, 'xtick.labelsize':annot_size, 'ytick.labelsize':annot_size})\n    # plt.rcParams['figure.figsize'] = fig_size\n    conf_df = pd.DataFrame(confuse)\n    conf_df.index = BIRDS\n    conf_df.columns = BIRDS\n    sns.heatmap(conf_df, annot=True, cmap='binary', annot_kws={'size': annot_size})\n    plt.title('Confusion Matrix', fontdict={'fontsize':annot_size}) \n    plt.xlabel('Predicted Birds', fontdict={'fontsize':annot_size})\n    plt.ylabel('Actual Birds', fontdict={'fontsize':annot_size})\n    plt.savefig('Confusion Matrix.png', dpi=200, format='png', bbox_inches='tight')\n\nconfusion_matrix_plot(model, test_batches)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classes with scores heatmap\nfrom sklearn.metrics import classification_report\ndef class_scores(classes_dir, model, test_batches):\n    \n    birds_train_count = {}\n    annot_size = 14\n    fig_size = (8,20)\n    \n    for f in os.listdir(classes_dir):\n        birds_train_count[f] = len(os.listdir(os.path.join(train_dir, f)))\n\n    birds_train_count = dict(sorted(birds_train_count.items(), key= lambda x: x[1], reverse=True))\n\n    preds = model.predict(x=test_batches)\n    y_preds = np.argmax(preds, axis=-1)\n    y_true = test_batches.classes\n    clf_report = classification_report(y_true, y_preds, output_dict=True)\n    cls_df = pd.DataFrame(clf_report).iloc[:-1, :].T\n    acc_to_counts = [str(BIRDS.index(bird)) for bird in [k for k in birds_train_count.keys()]] + ['accuracy', 'macro avg', 'weighted avg']\n\n    cls_df = cls_df.reindex(acc_to_counts)\n    y_label = [f\"{k} ({v})\" for k, v in birds_train_count.items()] + ['accuracy', 'macro avg', 'weighted avg']\n    cls_df.index = y_label\n\n    sns.set(rc = {'figure.figsize':fig_size})\n    sns.heatmap(cls_df, annot=True, annot_kws={'size': annot_size}, cmap='binary')\n    plt.title('Classes with Scores Heatmap') \n    plt.savefig('Classes with Scores Heatmap.png', dpi=200, format='png', bbox_inches='tight')\n\ntrain_dir = '/kaggle/input/birdfiltered/10SecDatasets/10SecDatasets/Train'\n\nclass_scores(train_dir, model_loaded, test_batches)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\ndir_name = '/kaggle/working/model/EFF_v2.keras'\noutput_filename = dir_name.split('/')[-1]\n\nshutil.make_archive(output_filename, 'zip', dir_name)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Accuracy and Loss \n\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nsns.set()\nsns.set_style(style='white')\n\n# Accuracy vs epochs plot\ndef accuracy_plot(model, history, epochs):\n\n    annot_size = 10\n    fig_size = (20, 20)\n\n    plt.plot(history.history['accuracy'], label='Training Accuracy', color='black')\n    plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy', linestyle='dashed', color='black')\n    \n    plt.title('Model Accuracy', fontsize=annot_size)\n    plt.xlabel('Epoch', fontsize=annot_size)\n    plt.ylabel('Accuracy', fontsize=annot_size)\n    plt.ylim([0, 1])\n    plt.xticks(list(range(1, epochs+1)))\n    plt.legend(loc='lower right', fontsize=annot_size)\n    plt.savefig('accuracy2.jpg', bbox_inches='tight')\n    plt.show()\n\n# Loss vs epochs plot\ndef loss_plot(model, history, epochs):\n    annot_size = 10\n    fig_size = (20, 20)\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.plot(loss, label='Training Loss', color='black')\n    plt.plot(val_loss, label = 'Validation Loss', linestyle='dashed', color='black')\n    \n    plt.title('Model Loss', fontsize=annot_size)\n    plt.xlabel('Epochs', fontsize=annot_size)\n    plt.ylabel('Loss', fontsize=annot_size)\n#     plt.ylim([0, 0.85])\n    plt.xticks(list(range(1, epochs+1)))\n    plt.legend(loc='upper right', fontsize=annot_size)\n    plt.savefig('loss2.jpg', bbox_inches='tight')\n    plt.show()\n\naccuracy_plot(model, history, epochs=15)\nloss_plot(model, history, epochs=15)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inference\n\n# !pip install pydub\nimport os\nimport numpy as np\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Rescaling\n\nfrom pydub import AudioSegment\nimport math\nimport shutil\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# # Turn interactive plotting off\n# import matplotlib\n# matplotlib.use('Agg')\n# plt.ioff()\n\n# Variables for feature extraction\nSAMPLE_RATE = 32000\nSPEC_SHAPE = (48, 128) # height x width\nSIGNAL_LENGTH = 10\nN_FFT = 1024         \nHOP_SIZE =  int(SIGNAL_LENGTH * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))      \nN_MELS = SPEC_SHAPE[0]   \nWIN_SIZE = 1024      \nWINDOW_TYPE = 'hann' \nFEATURE = 'mel'      \nFMIN = 500\nFMAX = 12500\n\ndef SplitAudio(audio_file, sec_to_split=10):\n    \n    validAudio = lambda audio, amp_threshold: True if audio.max > amp_threshold else False\n    \n    global temp_n\n    temp_n = 0\n    mili=1000\n    export_folder = f'./inference/tmp'\n    export_name = audio_file.split('/')[-1]\n    exportFormat = 'mp3'\n    try:\n        audio = AudioSegment.from_mp3(audio_file)\n    except:\n        print(\"File handled: \", audio_file)\n        extension = audio_file.split('.')[-1]\n        if extension == 'wav':\n            audio = AudioSegment.from_wav(audio_file)\n        elif extension == 'ogg':\n            audio = AudioSegment.from_ogg(audio_file)\n    th = audio.max//2\n \n    duration = math.floor(audio.duration_seconds)\n    splits = np.arange(0, duration, sec_to_split)\n\n    if duration > sec_to_split:\n        rem = duration % sec_to_split\n        n_splits = round(duration / sec_to_split)\n        for n in range(n_splits-1): # Except last splitted file\n            newAudio = audio[splits[n]*mili:splits[n+1]*mili]\n            temp_n = n\n            if validAudio(newAudio, th):\n                newAudio.export(f'{export_folder}/{export_name}{n}.{exportFormat}', format=exportFormat)\n        \n        # For the last splitted file\n        if rem <= sec_to_split//2:\n            n = temp_n\n            # If remaining  audio is small enough, don't separate\n            newAudio = audio[splits[n+1]*mili:duration*mili]\n            if validAudio(newAudio, th):\n                newAudio.export(f'{export_folder}/{export_name}{n+1}.{exportFormat}', format=exportFormat)\n\n        else:\n            n = temp_n\n            # If remaining audio is big enough, make it separate file\n            newAudio = audio[splits[n+1]*mili:duration*mili]\n            if validAudio(newAudio, th):\n                newAudio.export(f'{export_folder}/{export_name}{n+1}.{exportFormat}', format=exportFormat)\n\n    else:\n        if validAudio(audio, th):\n            audio.export(f'{export_folder}/{export_name}original.{exportFormat}', format=exportFormat)\n\ndef create_features(inference_data_path):\n    \n    # Find the list of all 10 sec splitted audio files\n    audio_files = [f for f in os.listdir(inference_data_path) if f.split('.')[-1]=='mp3']\n    count = 0\n    for aud in audio_files:\n\n        signal, sr = librosa.load(os.path.join(inference_data_path, aud),duration=10) # sr = sampling rate\n        # Plot mel-spectrogram\n        S = librosa.feature.melspectrogram(y=signal,\n                                           sr=SAMPLE_RATE,\n                                            n_fft=N_FFT,\n                                            hop_length=HOP_SIZE, \n                                            n_mels=N_MELS, \n                                            fmin=FMIN, \n                                            fmax=FMAX) \n\n        fig = plt.figure(figsize=(10, 4))\n        mel_spec = librosa.power_to_db(S, ref=np.max) \n        librosa.display.specshow(mel_spec, fmin=FMIN,y_axis='linear')\n        \n        plt.axis(False)\n        plt.tight_layout()\n        # plt.show()\n        count += 1\n        plt.savefig(f'{inference_data_path}/inf{count}.jpg')\n        plt.close(fig)\n        \ndef preprocess_img(image_dir, img_size):\n    img_width, img_height = img_size[0], img_size[1]\n    img = image.load_img(image_dir, target_size = (img_width, img_height))\n    img = image.img_to_array(img)\n    img = np.expand_dims(img, axis = 0)\n    norm = Rescaling(1./255)\n    img = norm(img)\n    img = preprocess_input(img) \n    return img\n\ndef prediction(model_dir, inference_data_path, img_size):\n    # List of feature images\n    feat = [os.path.join(inference_data_path, f) for f in os.listdir(inference_data_path) if f.split('.')[-1]=='jpg']\n    results = []\n    for m in feat:\n        model_loaded = load_model(model_dir)\n        img = preprocess_img(m, img_size)\n        output = BIRDS[np.argmax(model_loaded.predict(img), axis=-1)[0]]\n        results.append(output)\n    result = max(results)\n    \n    # Remove the temporary splitted audio and mfcc images\n    for i in os.listdir(inference_data_path):\n        os.remove(os.path.join(inference_data_path, i))\n    return results","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_data_path = './inference/tmp'\nraw_data_path = '/kaggle/input/birdfiltered/XC700260 - Great Hornbill - Buceros bicornis.mp3'\nmodel_dir = '/kaggle/working/model/CNMelSpec_Model'\nimg_size = (128, 128)\n\nif 'inference' not in os.listdir('./'):\n    os.makedirs(inference_data_path)\n\nSplitAudio(raw_data_path)\ncreate_features(inference_data_path)\nresult = prediction(model_dir, inference_data_path, img_size)\nprint(max(result))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-10T06:58:28.820Z"}},"outputs":[],"execution_count":null}]}